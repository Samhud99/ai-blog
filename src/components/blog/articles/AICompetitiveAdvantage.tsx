export function AICompetitiveAdvantage() {
  return (
    <>
      <p>
        <em>Written by Sam Hudson, with research and editing assistance from AI</em>
      </p>

      <p>
        In my last article, I argued that code complexity is becoming a less
        reliable moat. AI has reduced the cost of building software, and that
        gives businesses and consumers more options than they've had before.
        Replacing a vendor, building it yourself, or negotiating a better deal.
      </p>

      <p>
        This article will aim to focus more on the "why." Why are some moats
        eroding while others remain?
      </p>

      <p>
        I think the answer comes down to what's actually protecting them. If the
        only thing protecting a moat was that intelligence was scarce and
        expensive, AI is removing that protection. If it's protected by
        something else, it's probably more durable.
      </p>

      <h2>Limiting Factors of Intelligence</h2>

      <p>
        Dario Amodei (CEO of Anthropic) has a framework that I think helps make
        sense of this. In <em>Machines of Loving Grace</em> and{" "}
        <em>The Adolescence of Technology</em>, he argues that intelligence is
        only one input into progress.<sup>1</sup> Even as AI makes intelligence
        more abundant and accessible, five other factors remain as bottlenecks:
      </p>

      <ol>
        <li>
          <strong>Speed of the outside world.</strong> Physical things take
          time. Manufacturing, experiments, institutional change. You can't
          think your way past this.
        </li>
        <li>
          <strong>Need for data.</strong> If the data doesn't exist, more
          intelligence doesn't help. You have to go collect it, and that takes
          time too.
        </li>
        <li>
          <strong>Intrinsic complexity.</strong> Some systems are chaotic. Human
          behaviour, brand perception, social dynamics. Intelligence only gets
          you marginally better predictions in these systems.
        </li>
        <li>
          <strong>Constraints from humans.</strong> Regulation, law, habits,
          willingness to change. Plenty of things that work technically are held
          back by human factors. Amodei cites nuclear power and supersonic
          flight as examples.
        </li>
        <li>
          <strong>Physical laws.</strong> Hard ceilings. These don't move.
        </li>
      </ol>

      <p>
        These five factors are not about intelligence. They're about everything
        else. As AI reduces the intelligence bottleneck, these remain regardless
        of how capable the models get.
      </p>

      <p>
        This suggests a useful way to think about any competitive advantage:
        what is actually protecting it? If the main thing protecting it was that
        intelligence was scarce and expensive (it was hard to build, it required
        rare talent, it took clever engineering), that protection may weaken
        over time. If it's protected by one of Amodei's five limiting factors,
        it's probably more durable.
      </p>

      <p>
        Coding is where this shows up first, because it's one of the few
        valuable economic activities that isn't really constrained by any of the
        other five bottlenecks. No physical processes to wait for, no regulatory
        gates, no dependence on scarce real-world data. Nearly pure intelligence
        work. Which is probably why AI has improved at it so fast, and why
        software is worth looking at as an early indicator for other industries.
      </p>

      <h2>Competitive Advantage</h2>

      <p>
        Hamilton Helmer's <em>7 Powers</em> is the standard framework for
        durable competitive advantage.<sup>2</sup> Scale economies, network
        economies, counter-positioning, switching costs, branding, cornered
        resource, and process power.
      </p>

      <p>
        When you hold these up against Amodei's limiting factors, a pattern
        shows up. The barriers that depend primarily on intelligence scarcity
        (expensive R&D, complex integrations, large engineering teams) are the
        ones under most pressure. AI reduces the cost of R&D, writes migration
        scripts in hours, and increases leverage per engineer to the point where
        headcount matters less.
      </p>

      <p>
        The barriers protected by the other five factors look more durable.
        Brand trust is protected by intrinsic complexity. Regulatory approvals,
        contractual lock-in, and organisational habits are all protected by
        constraints from humans. Proprietary data is protected by the need for
        data and speed of the outside world. You can't synthesise ten years of
        real customer behaviour, and SOC 2 compliance takes years regardless of
        how capable the AI is.
      </p>

      <p>
        Counter-positioning is an interesting exception. It may actually be
        strengthened by AI. New entrants can build competitive alternatives
        faster than ever. But the incumbent's barriers to responding (revenue
        considerations, board expectations, organisational identity) are all
        constraints from humans. The offence gets faster while the defence stays
        roughly the same speed.
      </p>

      <p>
        The pattern that starts to emerge isn't really about individual powers
        holding up or not. It's about who is positioned to take advantage of the
        shifts across all of them.
      </p>

      <p>
        And that might be the more interesting question. Not "which powers hold
        up?" but "who is best at using AI to take advantage of the changes
        happening across all of them?"
      </p>

      <p>
        A company that can use AI to move faster, build alternatives quickly,
        accumulate data advantages, and deliver quality at speed isn't relying
        on any single power. It's working across the shifts between them.
      </p>

      <h2>Implications Beyond Silicon Valley</h2>

      <p>
        If organisational AI capability does turn out to be a meaningful
        advantage, it's worth noting that it doesn't belong exclusively to
        Silicon Valley firms or startups.
      </p>

      <p>
        Any organisation that develops strong AI practices could start building
        its own tools where it makes sense. Not replacing every SaaS
        subscription overnight. But having the option to, in specific areas, and
        knowing that the option exists.
      </p>

      <p>
        That's part of what makes option B (build it yourself) from Part 1 worth
        considering. Even if a company never fully exercises that option, being
        able to changes the conversation.
      </p>

      <p>
        Enterprise software is deeply embedded, and there are good reasons
        (reliability, security, support, integration depth) why organisations
        pay for established products. None of that is going away. But the range
        of options available to buyers is gradually widening.
      </p>

      <h2>Looking Ahead</h2>

      <p>
        In the long run, whether it's a new entrant, an incumbent, a supplier,
        or a customer, the companies that treat AI as an opportunity will
        outperform those that treat it as a threat to manage.
      </p>

      <p>
        By "treat it as an opportunity," I don't just mean adopting AI tools. I
        mean understanding how AI changes the competitive landscape around your
        business, and being agile enough to respond or disrupt when it arrives.
        Right now, software development is the clearest example. But this
        dynamic will likely reach other industries as AI capability improves,
        just at different speeds depending on how much Amodei's limiting factors
        constrain them.
      </p>

      <p>
        Every situation is different, but Helmer's <em>7 Powers</em> provides a
        useful starting point for thinking through which of your moats are
        protected by something durable, and which ones were only ever protected
        by the scarcity of intelligence.
      </p>

      <p>
        For SaaS companies, that question is already urgent. Understanding which
        of your moats actually hold up, and shifting toward the ones that do, is
        not something that can wait. For companies in other industries, there's
        more time, but the pattern is the same one we've seen with every major
        technology shift. The organisations that embraced the internet early
        didn't just survive the transition, they shaped it. AI is likely no
        different.
      </p>

      <hr />

      <h3>Sources:</h3>
      <ol>
        <li>
          Dario Amodei, <em>Machines of Loving Grace</em> (October 2024) and{" "}
          <em>The Adolescence of Technology</em> (January 2026)
        </li>
        <li>
          Hamilton Helmer, <em>7 Powers: The Foundations of Business Strategy</em>
        </li>
      </ol>

      <h3>More interesting reading:</h3>
      <ul>
        <li>
          Ben Thompson, <em>Microsoft and Software Survival</em> (February 2026)
          — on AI-written code and the long-term economics of software companies
        </li>
        <li>
          Janelle Teng Wade, <em>The SaaSacre of 2026</em> (February 2026) — a
          VC's breakdown of the AI-related fears driving the current software
          selloff
        </li>
        <li>
          a16z, <em>From Demos to Deals: Insights for Building in Enterprise AI</em>{" "}
          (July 2025) — on where moats actually form in enterprise AI
        </li>
        <li>
          Christoph Janz,{" "}
          <em>Some Learnings from Vibe Coding a Knowledge Hub in 13 Days</em>{" "}
          (January 2026) — a VC builds a 43,000 line system without writing
          code, and reflects on what it means for SaaS
        </li>
        <li>
          SaaStr, <em>The 2026 SaaS Crash: It's Not What You Think</em> (February
          2026) — argues the crash is real but the "AI kills SaaS" narrative is
          mostly wrong
        </li>
        <li>
          Margin of Safety,{" "}
          <em>SaaSpocalypse, Vibe Coding, and the New Scarcity</em> (February
          2026) — on accountability, maintenance, and the limits of vibe coding
          in enterprise
        </li>
        <li>
          Les Barclays,{" "}
          <em>Who captures the value when AI inference costs trend to zero?</em>
        </li>
      </ul>

      <hr />

      <p>
        <em>
          The views and opinions expressed in this article are solely those of
          the author and do not represent, reflect, or constitute the official
          position of any organization, company, or entity with which the author
          is or has been affiliated. The content herein is provided for general
          informational purposes only.
        </em>
      </p>
    </>
  );
}
